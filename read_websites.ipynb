{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Set the starting URL\n",
    "base_url = 'https://python.langchain.com/en/latest/index.html'\n",
    "\n",
    "# Initialize the set of visited URLs and add the base URL\n",
    "visited_urls = set()\n",
    "visited_urls.add(base_url)\n",
    "\n",
    "# Initialize the set of URLs to be visited and add the base URL\n",
    "urls_to_visit = set()\n",
    "urls_to_visit.add(base_url)\n",
    "\n",
    "# Parse the base URL to get the domain name\n",
    "base_domain = urlparse(base_url).netloc\n",
    "\n",
    "# Initialize the CSV writer\n",
    "csv_file = open('links.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the URLs to be visited\n",
    "while urls_to_visit:\n",
    "\n",
    "    # Get the next URL from the set of URLs to be visited\n",
    "    current_url = urls_to_visit.pop()\n",
    "\n",
    "    try:\n",
    "        # Make a GET request to the URL\n",
    "        response = requests.get(current_url)\n",
    "        print(response)\n",
    "\n",
    "        # Check if the response was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all the links on the page\n",
    "            for link in soup.find_all('a'):\n",
    "                print(link)\n",
    "\n",
    "                # Get the URL from the link\n",
    "                link_url = link.get('href')\n",
    "\n",
    "                # Make sure the link URL is not None and is not an empty string\n",
    "                if link_url is not None and link_url != '':\n",
    "                    print(link_url)\n",
    "                    # Normalize the link URL by joining it with the base URL\n",
    "                    link_url = urljoin(current_url, link_url)\n",
    "\n",
    "                    # Parse the domain name from the link URL\n",
    "                    link_domain = urlparse(link_url).netloc\n",
    "\n",
    "                    # Check if the link domain matches the base domain\n",
    "                    if link_domain == base_domain:\n",
    "\n",
    "                        # Add the link URL to the set of URLs to be visited if it hasn't been visited yet\n",
    "                        if link_url not in visited_urls:\n",
    "                            urls_to_visit.add(link_url)\n",
    "\n",
    "                        # Add the link URL to the set of visited URLs\n",
    "                        visited_urls.add(link_url)\n",
    "\n",
    "                        # Write the link URL to the CSV file\n",
    "                        csv_writer.writerow([link_url])\n",
    "\n",
    "        # Print an error message if the response was not successful\n",
    "        else:\n",
    "            print('Error: ' + str(response.status_code))\n",
    "\n",
    "    # Print an error message if there was an error making the GET request\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error: ' + str(e))\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "url_set = set()\n",
    "no_of_duplicates = 0\n",
    "\n",
    "with open('links.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    print(f\"no of urls found {len(list(reader))}\")\n",
    "    for row in reader:\n",
    "        url = row[0].strip()\n",
    "        if url in url_set:\n",
    "            no_of_duplicates += 1\n",
    "            print(f\"Duplicate URL found: {url}\")\n",
    "        else:\n",
    "            url_set.add(url)\n",
    "\n",
    "print(f\"No of duplicates found {no_of_duplicates}\")\n",
    "print(\"Duplicate check complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Create the data_output directory if it does not already exist\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open('links.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        url = row[0]\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if response object is not None\n",
    "        if response is not None:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_tag = soup.find('main', id='main-content', class_='bd-main')\n",
    "            if main_tag is not None:\n",
    "                main_text = main_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "                # Create a filename based on the url\n",
    "                filename = url.replace('https://', '').replace('http://', '').replace('/', '_').replace(':', '') + '.txt'\n",
    "\n",
    "                # Save the text to a file in the data_output directory\n",
    "                with open('output/' + filename, 'w') as f:\n",
    "                    print(counter)\n",
    "                    counter += 1\n",
    "                    f.write(main_text)\n",
    "            else:\n",
    "                print(f\"No main tag found in {url}\")\n",
    "        else:\n",
    "            print(f\"Error fetching {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
